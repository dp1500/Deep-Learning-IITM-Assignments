{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJxvFAtYRjrs5adthuOvWF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dp1500/Deep-Learning-IITM-Assignments/blob/main/GA3-V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBYPQNkCtKLG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSUsUfRIH0FW",
        "outputId": "2f9f98f3-7379-4c6a-9e1b-b4249a678dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vπ(high) = 0.73\n"
          ]
        }
      ],
      "source": [
        "gamma = 0.9\n",
        "threshold = 1e-5\n",
        "\n",
        "v_high = 0  # Initial estimate for v(high)\n",
        "\n",
        "while True:\n",
        "    v_high_new = 0.2 * (3 + gamma * v_high)  # Update v(high)\n",
        "\n",
        "    # Check for convergence\n",
        "    if abs(v_high_new - v_high) < threshold:\n",
        "        break\n",
        "\n",
        "    v_high = v_high_new  # Update the value estimate\n",
        "\n",
        "# Round the result to two decimal places\n",
        "v_high = round(v_high, 2)\n",
        "\n",
        "print(\"vπ(high) =\", v_high)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9\n",
        "threshold = 1e-5\n",
        "\n",
        "v_low = 0  # Initial estimate for v(low)\n",
        "\n",
        "while True:\n",
        "    v_low_new = 1 * (0 + gamma * v_low)  # Update v(low)\n",
        "\n",
        "    # Check for convergence\n",
        "    if abs(v_low_new - v_low) < threshold:\n",
        "        break\n",
        "\n",
        "    v_low = v_low_new  # Update the value estimate\n",
        "\n",
        "# Round the result to two decimal places\n",
        "v_low = round(v_low, 2)\n",
        "\n",
        "print(\"vπ(low) =\", v_low)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwTmUc3aH0yv",
        "outputId": "9f3b903a-350a-4942-b7c5-76dfbbed2732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vπ(low) = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9\n",
        "threshold = 1e-5\n",
        "\n",
        "v_low = 0  # Initial estimate for v(low)\n",
        "\n",
        "while True:\n",
        "    v_low_new = 0.1 * (0 + gamma * v_low) + 0.9 * (0 + gamma * v_low)  # Update v(low)\n",
        "\n",
        "    # Check for convergence\n",
        "    if abs(v_low_new - v_low) < threshold:\n",
        "        break\n",
        "\n",
        "    v_low = v_low_new  # Update the value estimate\n",
        "\n",
        "# Round the result to two decimal places\n",
        "v_low = round(v_low, 2)\n",
        "\n",
        "print(\"vπ(low) =\", v_low)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzuy70uZIA8w",
        "outputId": "d83138e3-5675-4116-b708-3a86e3128f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vπ(low) = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the weights and biases\n",
        "files = np.load('/content/parameters.npz')\n",
        "W1 = files.get('W1')\n",
        "W2 = files.get('W2')\n",
        "W3 = files.get('W3')\n",
        "b1 = files.get('b1')\n",
        "b2 = files.get('b2')\n",
        "b3 = files.get('b3')\n",
        "\n",
        "# Input vector\n",
        "x = np.array([1, 0, 1]).reshape(3, 1)\n",
        "\n",
        "# Expected labels\n",
        "y = np.array([0, 0, 1])\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Softmax activation function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z))  # Subtract max(z) for numerical stability\n",
        "    return exp_z / exp_z.sum()\n",
        "\n",
        "# Forward pass\n",
        "a1 = np.dot(W1, x) + b1\n",
        "h1 = sigmoid(np.dot(W1, x) + b1)\n",
        "\n",
        "a2 = np.dot(W2, a1) + b2\n",
        "h2 = sigmoid(np.dot(W2, a1) + b2)\n",
        "\n",
        "\n",
        "a3 = np.dot(W3, a2) + b3\n",
        "h3 = softmax(np.dot(W3, a2) + b3)\n",
        "\n",
        "# Calculate cross-entropy loss\n",
        "loss = -np.sum(y * np.log(a3))\n",
        "\n",
        "print(\"Predicted output:\", a3)\n",
        "print(\"Cross-entropy loss:\", loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmSr92_SIL5N",
        "outputId": "92be0590-fd7e-4115-bf2e-10fa8216cee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted output: [[3.23544033]\n",
            " [6.94803277]\n",
            " [5.8668836 ]]\n",
            "Cross-entropy loss: -4.881947188257447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLPncAOfTxRr",
        "outputId": "681cc4fb-6bed-4d0e-c4c8-e16182b104e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a1 = sigmoid(np.dot(W1, x) + b1)\n"
      ],
      "metadata": {
        "id": "WCzQTHFJaIwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPkP5PUZctJg",
        "outputId": "ea129a12-88eb-45c8-ccb0-cc46ec53fd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.5350184 ]\n",
            " [1.98250233]\n",
            " [1.93014489]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a1.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tce3NBecuLN",
        "outputId": "a91aa5df-d586-486a-dd18-80aa2abd6dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.44766562448759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htdmYXYJdWNt",
        "outputId": "3a6eb417-15d0-4ba5-bcb7-50c4000549b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.82273938],\n",
              "       [0.87894766],\n",
              "       [0.87326546]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(h1.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PMRuviYd9w7",
        "outputId": "5c6e10ca-1ab8-47f5-bcbb-152160659a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5749524957231924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for x in [a2,h2,a3]:\n",
        "  l.append(x.sum())"
      ],
      "metadata": {
        "id": "PKCn0AeXeATR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q1h0AhEeVCY",
        "outputId": "de193ec1-c4a8-4910-9403-de5d3ae5b47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11.474299614338914, 2.8747650858050005, 16.050356702372174]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given predicted output probabilities\n",
        "a3 = np.array([[3.23544033], [6.94803277], [5.8668836]])\n",
        "\n",
        "# True labels\n",
        "y = np.array([0, 0, 1])  # Assuming one-hot encoded labels\n",
        "\n",
        "# Calculate cross-entropy loss\n",
        "loss = -np.sum(y * np.log(a3))\n",
        "\n",
        "# Print the loss value\n",
        "print(\"Cross-entropy loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiNMmTm6eWFq",
        "outputId": "7f1ecdd7-8ceb-4f1d-c633-94a92a489024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-entropy loss: -4.881947187862163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Work"
      ],
      "metadata": {
        "id": "AFRGwUwBM3MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Deep learning problem\n",
        "\n",
        "# Input layer\n",
        "# x= [[1], [0], [1]]\n",
        "x = np.array([[1], [0], [1]])\n",
        "\n",
        "# Output layer\n",
        "# y = [[0], [0], [1]]\n",
        "y = np.array([[0], [0], [1]])\n",
        "\n",
        "\n",
        "# Parameters\n",
        "files = np.load('parameters.npz')\n",
        "\n",
        "# Weights\n",
        "\n",
        "w1 = files.get('W1')\n",
        "w2 = files.get('W2')\n",
        "w3 = files.get('W3')\n",
        "\n",
        "# Biases\n",
        "b1 = files.get('b1')\n",
        "b2 = files.get('b2')\n",
        "b3 = files.get('b3')\n",
        "\n",
        "# w1 = \t​[[0.5488135 , 0.71518937, 0.60276338], [0.54488318, 0.4236548 , 0.64589411], [0.43758721, 0.891773  , 0.96366276]]\n",
        "# w2 = [[0.56804456, 0.92559664, 0.07103606], [0.0871293 , 0.0202184 , 0.83261985], [0.77815675, 0.87001215, 0.97861834]]\n",
        "# w3 = [[0.11827443, 0.63992102, 0.14335329], [0.94466892, 0.52184832, 0.41466194], [0.26455561, 0.77423369, 0.45615033]]\n",
        "\n",
        "# b1 = [[0.38344152], [0.79172504], [0.52889492]]\n",
        "# b2 = [[​0.79915856, 0.46147936, 0.78052918]]\n",
        "# b3 = [[0.56843395], [0.0187898 ], [0.6176355 ]]\n",
        "\n",
        "def forward_propagation():\n",
        "    # Forward propagation\n",
        "\n",
        "    # a represents the pre-activation of a layer\n",
        "    # h represents the activation of a layer. h = g(a) where g is a sigmoid function\n",
        "\n",
        "    # Hidden layer 1\n",
        "\n",
        "    # Pre-activation\n",
        "    a1 = np.dot(w1, x) + b1\n",
        "    print(\"a1 = \", a1)\n",
        "    print(\"Sum of a1 = \", np.sum(a1))\n",
        "\n",
        "    # Activation\n",
        "    h1 = 1 / (1 + np.exp(-a1))\n",
        "\n",
        "    print('Hidden layer 1 activation: ', h1)\n",
        "    print('Sum of hidden layer 1 activation: ', np.sum(h1))\n",
        "\n",
        "\n",
        "    # Hidden layer 2\n",
        "\n",
        "    # Pre-activation\n",
        "    a2 = np.dot(w2, h1) + b2\n",
        "    print(\"a2 = \", a2)\n",
        "    print(\"Sum of a2 = \", np.sum(a2))\n",
        "\n",
        "    # Activation\n",
        "    h2 = 1 / (1 + np.exp(-a2))\n",
        "\n",
        "    print('Hidden layer 2 activation: ', h2)\n",
        "    print('Sum of hidden layer 2 activation: ', np.sum(h2))\n",
        "\n",
        "    # Output layer\n",
        "\n",
        "    # Pre-activation\n",
        "    a3 = np.dot(w3, h2) + b3\n",
        "    print(\"a3 = \", a3)\n",
        "    print(\"Sum of a3 = \", np.sum(a3))\n",
        "\n",
        "    # Activation - Using softmax function\n",
        "    y_hat = h3 = np.exp(a3) / np.sum(np.exp(a3))\n",
        "\n",
        "\n",
        "    print('Output layer pre-activation: ', a3)\n",
        "    print('Sum of output layer pre-activation: ', np.sum(a3))\n",
        "\n",
        "    print('Output layer activation: ', h3)\n",
        "    print('Sum of output layer activation: ', np.sum(h3))\n",
        "\n",
        "\n",
        "    print(\"Sum of elements a2, h2, a3 are:\", np.sum(a2), np.sum(h2), np.sum(a3))\n",
        "\n",
        "    return h1, h2, y_hat\n",
        "\n",
        "h1, h2, y_hat = forward_propagation()\n",
        "\n",
        "# Loss function - Using cross entropy loss\n",
        "# L_theta = -ylog(y_hat)  # y_hat is the predicted value of y\n",
        "\n",
        "L_theta = -np.sum(y * np.log(y_hat))\n",
        "print(\"Cross Entropy Loss function: \", L_theta)\n",
        "\n",
        "\n",
        "# Backward propagation\n",
        "\n",
        "# Derivative of loss function with respect to a3\n",
        "# i.e Gradient of the loss function with respect to the pre-activation of the output layer\n",
        "\n",
        "del_L_theta_del_a3 =  - (y-y_hat) # one hot encoding of y - y_hat\n",
        "print(\"Gradient of the loss function with respect to the pre-activation of the output layer del_L_theta_del_a3: \", del_L_theta_del_a3)\n",
        "\n",
        "# k = 3\n",
        "\n",
        "# Derivative of loss function with respect to w3\n",
        "# i.e Gradient of the loss function with respect to the weights of the output layer\n",
        "\n",
        "del_L_theta_del_w3 = np.dot(del_L_theta_del_a3, h2.T)\n",
        "print(\"Gradient of the loss function with respect to the weights of the output layer del_L_theta_del_w3: \", del_L_theta_del_w3)\n",
        "\n",
        "# Derivative of loss function with respect to b3\n",
        "# i.e Gradient of the loss function with respect to the biases of the output layer\n",
        "\n",
        "del_L_theta_del_b3 = del_L_theta_del_a3\n",
        "print(\"Gradient of the loss function with respect to the biases of the output layer del_L_theta_del_b3: \", del_L_theta_del_b3)\n",
        "\n",
        "# Derivative of loss function with respect to h2\n",
        "# i.e Gradient of the loss function with respect to the activation of the hidden layer 2\n",
        "\n",
        "del_L_theta_del_h2 = np.dot(w3.T, del_L_theta_del_a3)\n",
        "print(\"Gradient of the loss function with respect to the activation of the hidden layer 2 del_L_theta_del_h2: \", del_L_theta_del_h2)\n",
        "\n",
        "# Derivative of loss function with respect to a2\n",
        "# i.e Gradient of the loss function with respect to the pre-activation of the hidden layer 2\n",
        "\n",
        "del_L_theta_del_a2 = del_L_theta_del_h2 * h2 * (1 - h2)\n",
        "print(\"Gradient of the loss function with respect to the pre-activation of the hidden layer 2 del_L_theta_del_a2: \", del_L_theta_del_a2)\n",
        "\n",
        "# k = 2\n",
        "\n",
        "# Derivative of loss function with respect to w2\n",
        "# i.e Gradient of the loss function with respect to the weights of the hidden layer 2\n",
        "\n",
        "del_L_theta_del_w2 = np.dot(del_L_theta_del_a2, h1.T)\n",
        "print(\"Gradient of the loss function with respect to the weights of the hidden layer 2 del_L_theta_del_w2: \", del_L_theta_del_w2)\n",
        "\n",
        "# Derivative of loss function with respect to b2\n",
        "\n",
        "del_L_theta_del_b2 = del_L_theta_del_a2\n",
        "\n",
        "print(\"Gradient of the loss function with respect to the biases of the hidden layer 2 del_L_theta_del_b2: \", del_L_theta_del_b2)\n",
        "\n",
        "\n",
        "# Derivative of loss function with respect to h1\n",
        "# i.e Gradient of the loss function with respect to the activation of the hidden layer 1\n",
        "\n",
        "del_L_theta_del_h1 = np.dot(w2.T, del_L_theta_del_a2)\n",
        "print(\"Gradient of the loss function with respect to the activation of the hidden layer 1 del_L_theta_del_h1: \", del_L_theta_del_h1)\n",
        "\n",
        "# Derivative of loss function with respect to a1\n",
        "# i.e Gradient of the loss function with respect to the pre-activation of the hidden layer 1\n",
        "\n",
        "del_L_theta_del_a1 = del_L_theta_del_h1 * h1 * (1 - h1)\n",
        "print(\"Gradient of the loss function with respect to the pre-activation of the hidden layer 1 del_L_theta_del_a1: \", del_L_theta_del_a1)\n",
        "\n",
        "# k = 1\n",
        "\n",
        "# Derivative of loss function with respect to w1\n",
        "# i.e Gradient of the loss function with respect to the weights of the hidden layer 1\n",
        "\n",
        "del_L_theta_del_w1 = np.dot(del_L_theta_del_a1, x.T)\n",
        "print(\"Gradient of the loss function with respect to the weights of the hidden layer 1 del_L_theta_del_w1: \", del_L_theta_del_w1)\n",
        "\n",
        "# Derivative of loss function with respect to b1\n",
        "\n",
        "del_L_theta_del_b1 = del_L_theta_del_a1\n",
        "print(\"Gradient of the loss function with respect to the biases of the hidden layer 1 del_L_theta_del_b1: \", del_L_theta_del_b1)\n",
        "\n",
        "# Updating the parameters\n",
        "\n",
        "eta = 1\n",
        "\n",
        "w1 = w1 - eta * del_L_theta_del_w1\n",
        "w2 = w2 - eta * del_L_theta_del_w2\n",
        "w3 = w3 - eta * del_L_theta_del_w3\n",
        "\n",
        "b1 = b1 - eta * del_L_theta_del_b1\n",
        "b2 = b2 - eta * del_L_theta_del_b2\n",
        "b3 = b3 - eta * del_L_theta_del_b3\n",
        "\n",
        "# Repeating the forward propagation\n",
        "\n",
        "h1, h2, y_hat = forward_propagation()\n",
        "\n",
        "# Repeating the loss function\n",
        "\n",
        "L_theta = -np.sum(y * np.log(y_hat))\n",
        "print(\"New Cross Entropy Loss function: \", L_theta)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ei2qCnaplBYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06557a4-4753-4c3e-a252-63bb53a03eae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a1 =  [[1.5350184 ]\n",
            " [1.98250233]\n",
            " [1.93014489]]\n",
            "Sum of a1 =  5.44766562448759\n",
            "Hidden layer 1 activation:  [[0.82273938]\n",
            " [0.87894766]\n",
            " [0.87326546]]\n",
            "Sum of hidden layer 1 activation:  2.5749524957231924\n",
            "a2 =  [[2.14209553]\n",
            " [1.27803313]\n",
            " [3.04003811]]\n",
            "Sum of a2 =  6.460166773282593\n",
            "Hidden layer 2 activation:  [[0.89492782]\n",
            " [0.78211479]\n",
            " [0.95435049]]\n",
            "Sum of hidden layer 2 activation:  2.63139309587371\n",
            "a3 =  [[1.311582  ]\n",
            " [1.66807841]\n",
            " [1.89526058]]\n",
            "Sum of a3 =  4.874920988265704\n",
            "Output layer pre-activation:  [[1.311582  ]\n",
            " [1.66807841]\n",
            " [1.89526058]]\n",
            "Sum of output layer pre-activation:  4.874920988265704\n",
            "Output layer activation:  [[0.23691422]\n",
            " [0.33838847]\n",
            " [0.42469732]]\n",
            "Sum of output layer activation:  1.0\n",
            "Sum of elements a2, h2, a3 are: 6.460166773282593 2.63139309587371 4.874920988265704\n",
            "Cross Entropy Loss function:  0.8563785622753882\n",
            "Gradient of the loss function with respect to the pre-activation of the output layer del_L_theta_del_a3:  [[ 0.23691422]\n",
            " [ 0.33838847]\n",
            " [-0.57530268]]\n",
            "Gradient of the loss function with respect to the weights of the output layer del_L_theta_del_w3:  [[ 0.21202113  0.18529411  0.2260992 ]\n",
            " [ 0.30283325  0.26465862  0.3229412 ]\n",
            " [-0.51485438 -0.44995274 -0.5490404 ]]\n",
            "Gradient of the loss function with respect to the biases of the output layer del_L_theta_del_b3:  [[ 0.23691422]\n",
            " [ 0.33838847]\n",
            " [-0.57530268]]\n",
            "Gradient of the loss function with respect to the activation of the hidden layer 2 del_L_theta_del_h2:  [[ 0.1954864 ]\n",
            " [-0.11722488]\n",
            " [-0.08814526]]\n",
            "Gradient of the loss function with respect to the pre-activation of the hidden layer 2 del_L_theta_del_a2:  [[ 0.01838198]\n",
            " [-0.01997644]\n",
            " [-0.0038401 ]]\n",
            "Gradient of the loss function with respect to the weights of the hidden layer 2 del_L_theta_del_w2:  [[ 0.01512358  0.0161568   0.01605235]\n",
            " [-0.0164354  -0.01755824 -0.01744473]\n",
            " [-0.0031594  -0.00337525 -0.00335343]]\n",
            "Gradient of the loss function with respect to the biases of the hidden layer 2 del_L_theta_del_b2:  [[ 0.01838198]\n",
            " [-0.01997644]\n",
            " [-0.0038401 ]]\n",
            "Gradient of the loss function with respect to the activation of the hidden layer 1 del_L_theta_del_h1:  [[ 0.00571305]\n",
            " [ 0.01326947]\n",
            " [-0.01908499]]\n",
            "Gradient of the loss function with respect to the pre-activation of the hidden layer 1 del_L_theta_del_a1:  [[ 0.00083319]\n",
            " [ 0.00141185]\n",
            " [-0.00211219]]\n",
            "Gradient of the loss function with respect to the weights of the hidden layer 1 del_L_theta_del_w1:  [[ 0.00083319  0.          0.00083319]\n",
            " [ 0.00141185  0.          0.00141185]\n",
            " [-0.00211219  0.         -0.00211219]]\n",
            "Gradient of the loss function with respect to the biases of the hidden layer 1 del_L_theta_del_b1:  [[ 0.00083319]\n",
            " [ 0.00141185]\n",
            " [-0.00211219]]\n",
            "a1 =  [[1.53251884]\n",
            " [1.97826677]\n",
            " [1.93648147]]\n",
            "Sum of a1 =  5.447267075406992\n",
            "Hidden layer 1 activation:  [[0.82237455]\n",
            " [0.87849628]\n",
            " [0.87396509]]\n",
            "Sum of hidden layer 1 activation:  2.5748359152999134\n",
            "a2 =  [[2.08247808]\n",
            " [1.34273818]\n",
            " [3.05238043]]\n",
            "Sum of a2 =  6.477596686163293\n",
            "Hidden layer 2 activation:  [[0.88918844]\n",
            " [0.79293987]\n",
            " [0.95488519]]\n",
            "Sum of hidden layer 2 activation:  2.63701349827328\n",
            "a3 =  [[0.5296402 ]\n",
            " [0.54263293]\n",
            " [3.8165285 ]]\n",
            "Sum of a3 =  4.888801638264937\n",
            "Output layer pre-activation:  [[0.5296402 ]\n",
            " [0.54263293]\n",
            " [3.8165285 ]]\n",
            "Sum of output layer pre-activation:  4.888801638264937\n",
            "Output layer activation:  [[0.03475536]\n",
            " [0.03520987]\n",
            " [0.93003478]]\n",
            "Sum of output layer activation:  1.0\n",
            "Sum of elements a2, h2, a3 are: 6.477596686163293 2.63701349827328 4.888801638264937\n",
            "New Cross Entropy Loss function:  0.07253330081047048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6yV8m6HM42x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}